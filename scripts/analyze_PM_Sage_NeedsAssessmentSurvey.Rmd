---
title: "Survey Analysis: Identifying and Prioritizing Project Management Needs"
output: html_document
---

```{r setup, include=FALSE}
# Install necessary libraries if not already installed
required_packages <- c(
  "tidytext", "dplyr", "factoextra", "cluster", "ggplot2", "pheatmap",
  "readr", "wordcloud", "RColorBrewer", "tm"
)

# Install missing packages
installed_packages <- rownames(installed.packages())
for (pkg in required_packages) {
  if (!(pkg %in% installed_packages)) {
    install.packages(pkg, dependencies = TRUE)
  }
}

# Load the packages after installation
lapply(required_packages, library, character.only = TRUE)



# Load necessary libraries
library(tidytext)
library(dplyr)
library(factoextra)  # Clustering validation and visualization
library(cluster)  # Clustering algorithms
library(ggplot2)  # Visualization
library(pheatmap)  # Heatmap visualization
library(readr)
library(wordcloud)
library(RColorBrewer)  # Color palettes for visualization
library(tm)  # For Document-Term Matrix
set.seed(123)

# 1. Load and clean data
survey_data <- read_csv("/Users/ryaxley/Documents/GitHub/modelad/docs/PM_Sage_NeedsAssessmentSurvey-Responses.csv")

clean_survey_data <- survey_data %>%
  mutate(`What is your title?` = case_when(
    is.na(`What is your title?`) ~ "Not Provided",  
    TRUE ~ `What is your title?`
  ))

# 2. Word Frequency Analysis

# Tokenize and remove stop words
text_data <- clean_survey_data %>%
  select(starts_with("What do you think"), starts_with("If you had"), starts_with("What is your #2")) %>%
  unnest_tokens(word, everything()) %>%
  anti_join(stop_words = tidytext::stop_words)

# Count most frequent words
common_words <- text_data %>%
  count(word, sort = TRUE) %>%
  filter(n > 5)

# Visualize word frequency with a bar chart
ggplot(common_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Most Common Project Management Needs", x = "Word", y = "Frequency")

# Generate a beautiful word cloud to illustrate themes
wordcloud(
  words = common_words$word, 
  freq = common_words$n, 
  min.freq = 5, 
  max.words = 100, 
  random.order = FALSE, 
  rot.per = 0.35, 
  scale = c(3, 0.5),  # Adjust word size
  colors = brewer.pal(8, "Dark2")  # Set color palette
)

# 3. Sentiment Analysis

# Load Bing sentiment lexicon
bing_sentiment <- get_sentiments("bing")

# Perform sentiment analysis by joining with Bing lexicon
sentiment_data <- text_data %>%
  inner_join(bing_sentiment, by = "word") %>%
  count(sentiment)

# Plot sentiment distribution
ggplot(sentiment_data, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentiment Distribution in Survey Responses", x = "Sentiment", y = "Word Count")

# 4. Clustering Analysis

# Create Document-Term Matrix (DTM)
dtm <- text_data %>%
  count(document = row_number(), word) %>%
  cast_dtm(document, word, n)

# Perform K-Means clustering
kmeans_result <- kmeans(as.matrix(dtm), centers = 3)

# Visualize clusters using PCA plot
fviz_cluster(kmeans_result, data = as.matrix(dtm), geom = "point", ellipse = TRUE) +
  labs(title = "Clustering of Project Management Needs")

# 5. Prioritize Clusters by Sentiment

# Calculate sentiment score per response
response_sentiment <- text_data %>%
  inner_join(bing_sentiment, by = "word") %>%
  group_by(document) %>%
  summarise(sentiment_score = sum(ifelse(sentiment == "positive", 1, -1)))

# Merge sentiment scores with the cleaned data and add cluster labels
clean_survey_data <- clean_survey_data %>%
  mutate(sentiment_score = response_sentiment$sentiment_score)

# Aggregate sentiment scores by cluster
cluster_sentiment <- clean_survey_data %>%
  mutate(cluster = kmeans_result$cluster) %>%
  group_by(cluster) %>%
  summarise(average_sentiment = mean(sentiment_score, na.rm = TRUE))

# Visualize sentiment by cluster
ggplot(cluster_sentiment, aes(x = factor(cluster), y = average_sentiment, fill = average_sentiment)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "red", high = "green") +
  labs(title = "Average Sentiment by Cluster", x = "Cluster", y = "Sentiment Score")

# 6. Concrete Actions and Save Results

# Sort clusters by average sentiment and print actionable insights
cluster_sentiment <- cluster_sentiment %>%
  arrange(average_sentiment)

cat("\nClusters Prioritized by Sentiment:\n")
print(cluster_sentiment)

# Save prioritized clusters by sentiment to CSV
write_csv(cluster_sentiment, "/Users/ryaxley/Documents/GitHub/modelad/docs/prioritized_clusters_by_sentiment.csv")
